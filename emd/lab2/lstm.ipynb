{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "pandas.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "\n",
    "def cleanup(tweets):\n",
    "    tweets = tweets.dropna()\n",
    "    tweets = tweets.drop(columns=['Id'])\n",
    "    tweets = tweets[tweets.Category != 'Tweet']\n",
    "    tweets = tweets[tweets.Tweet != 'Not Available']\n",
    "    tweets = tweets[tweets.Tweet != '']\n",
    "    return tweets\n",
    "\n",
    "import re\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# stops = set(stopwords.words('english'))\n",
    "stops = {'in', 'of', 'at', 'a', 'the', 'to', 'on', 'and', 'it'}\n",
    "stops.update(string.punctuation)\n",
    "stops.difference_update('?!')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tag_for_lemmatizer(tag):\n",
    "    if tag.startswith('NN'):\n",
    "        return 'n'\n",
    "    if tag.startswith('VB'):\n",
    "        return 'v'\n",
    "    return 'a'\n",
    "\n",
    "def preprocess(text, lemmatize=True):\n",
    "    if not text or type(text) != str:\n",
    "        return ''\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"https?://[^\\s]+\", '', text) # hyperlinks\n",
    "    text = re.sub(r\"\\@\\w+\", '', text) # mentions\n",
    "    text = re.sub(r\"#\", '', text) # hashtags\n",
    "    text = re.sub(r\"\\d+\\w*\", '', text) # numbers\n",
    "    text = re.sub(r\"'s\", '', text) # possesive\n",
    "    text = re.sub(r\"n't\", ' not', text) # contractions\n",
    "    \n",
    "    words = [word for word in casual_tokenize(text) if word not in stops]\n",
    "    \n",
    "    if lemmatize:\n",
    "        words = [\n",
    "            lemmatizer.lemmatize(word, tag_for_lemmatizer(tag))\n",
    "            for word, tag in pos_tag(words)\n",
    "        ]\n",
    "    else:\n",
    "        words = [\n",
    "            stemmer.stem(word)\n",
    "            for word in words\n",
    "        ]\n",
    "    text = ' '.join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>io app transport security mm need check if my party network pod support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>mar if you have an ios device you should download our app too</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>my phone do not run late io which may account for problem other day .. time be replace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>not sure how start your publication io ? we'll be live help with ask me anything session today friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>neutral</td>\n",
       "      <td>two dollar tuesday be here with forklift quickkey for io suite for page for just today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5963</th>\n",
       "      <td>positive</td>\n",
       "      <td>ok ed let do this zlatan greizmann laporte tomorrow make happen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5964</th>\n",
       "      <td>neutral</td>\n",
       "      <td>goal level zlatan by friday ? post every other day dsgs vine by</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5965</th>\n",
       "      <td>neutral</td>\n",
       "      <td>would not surprise me if we enquired.he ca not be happy play fiddle zlatan but he not worth psg ask price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5966</th>\n",
       "      <td>neutral</td>\n",
       "      <td>rib injury for zlatan against russia be big blow if he miss austria game tuesday chance for new sunderland striker toivonen safc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5967</th>\n",
       "      <td>neutral</td>\n",
       "      <td>noooooo ! i be hop see zlatan be zlatan tuesday ! oh well still look forward match</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5421 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Category  \\\n",
       "1      neutral   \n",
       "2      neutral   \n",
       "3     negative   \n",
       "4     positive   \n",
       "5      neutral   \n",
       "...        ...   \n",
       "5963  positive   \n",
       "5964   neutral   \n",
       "5965   neutral   \n",
       "5966   neutral   \n",
       "5967   neutral   \n",
       "\n",
       "                                                                                                                                 Tweet  \n",
       "1                                                              io app transport security mm need check if my party network pod support  \n",
       "2                                                                        mar if you have an ios device you should download our app too  \n",
       "3                                               my phone do not run late io which may account for problem other day .. time be replace  \n",
       "4                                not sure how start your publication io ? we'll be live help with ask me anything session today friday  \n",
       "5                                               two dollar tuesday be here with forklift quickkey for io suite for page for just today  \n",
       "...                                                                                                                                ...  \n",
       "5963                                                                   ok ed let do this zlatan greizmann laporte tomorrow make happen  \n",
       "5964                                                                   goal level zlatan by friday ? post every other day dsgs vine by  \n",
       "5965                         would not surprise me if we enquired.he ca not be happy play fiddle zlatan but he not worth psg ask price  \n",
       "5966  rib injury for zlatan against russia be big blow if he miss austria game tuesday chance for new sunderland striker toivonen safc  \n",
       "5967                                                noooooo ! i be hop see zlatan be zlatan tuesday ! oh well still look forward match  \n",
       "\n",
       "[5421 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load train set\n",
    "\n",
    "tweets = pandas.read_csv('data/train.csv')\n",
    "tweets = cleanup(tweets)\n",
    "tweets.Tweet = tweets.Tweet.apply(preprocess, lemmatize=True)\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# convert text to sequences\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(tweets.Tweet)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(tweets.Tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,  909, 4082,  284],\n",
       "       [   0,    0,    0, ...,  123,  419,  212],\n",
       "       [   0,    0,    0, ...,   24,    1,  910],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  652,  289,  760],\n",
       "       [   0,    0,    0, ..., 1021, 9218, 9219],\n",
       "       [   0,    0,    0, ...,   66,  465,  335]], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pad sequences\n",
    "\n",
    "lengths = [len(seq) for seq in sequences]\n",
    "plt.hist(lengths)\n",
    "plt.show()\n",
    "\n",
    "max_words = 30\n",
    "sequences = sequence.pad_sequences(sequences, maxlen=max_words)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert categories to one-hot vectors\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "y = ohe.fit_transform(tweets.Category.values.reshape(-1, 1))\n",
    "num_classes = len(ohe.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(sequences, y, test_size=0.2)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_24 (Embedding)     (None, 30, 100)           922100    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_20 (Spatia (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_24 (LSTM)               (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 1,002,803\n",
      "Trainable params: 1,002,803\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikolaj/Code/iti2/emd/lab2/.venv/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3468 samples, validate on 868 samples\n",
      "Epoch 1/3\n",
      "3468/3468 [==============================] - 4s 1ms/step - loss: 1.0249 - accuracy: 0.4686 - val_loss: 0.9695 - val_accuracy: 0.5207\n",
      "Epoch 2/3\n",
      "3468/3468 [==============================] - 6s 2ms/step - loss: 0.9578 - accuracy: 0.5202 - val_loss: 0.8511 - val_accuracy: 0.6025\n",
      "Epoch 3/3\n",
      "3468/3468 [==============================] - 8s 2ms/step - loss: 0.8297 - accuracy: 0.6130 - val_loss: 0.8424 - val_accuracy: 0.6002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x17894df28>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup and train neural network\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Embedding, SpatialDropout1D, LSTM, Dense\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=max_words),\n",
    "    SpatialDropout1D(0.5),\n",
    "    LSTM(100, dropout=0.5, recurrent_dropout=0.5),\n",
    "    Dense(num_classes, activation='softmax'),\n",
    "])\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "epochs = 3\n",
    "batch_size = 32\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          validation_data=(x_valid, y_valid),\n",
    "          epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 48  92  33]\n",
      " [ 46 206 151]\n",
      " [ 13 118 378]]\n",
      "0.5824884792626728\n"
     ]
    }
   ],
   "source": [
    "# check against the test set\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "predictions = model.predict_classes(x_test)\n",
    "truth = y_test.argmax(1)\n",
    "\n",
    "print(confusion_matrix(truth, predictions))\n",
    "print(accuracy_score(truth, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I loved it!': 'positive',\n",
       " \"I don't know what to say\": 'neutral',\n",
       " 'What a fucking piece of shit': 'negative'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test on our own input\n",
    "\n",
    "def classify(text):\n",
    "    text = preprocess(text)\n",
    "    test_seq = tokenizer.texts_to_sequences([text])\n",
    "    test_seq = sequence.pad_sequences(test_seq, maxlen=max_words)\n",
    "    predictions = model.predict_classes(test_seq)\n",
    "    return ohe.categories_[0][predictions[0]]\n",
    "\n",
    "phrases = [\"I loved it!\", \"I don't know what to say\", \"What a fucking piece of shit\"]\n",
    "{ phrase: classify(phrase) for phrase in phrases }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict categories on the external test set\n",
    "\n",
    "def classify_array(texts):\n",
    "    texts = [preprocess(text) for text in texts]\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    sequences = sequence.pad_sequences(sequences, maxlen=max_words)\n",
    "    predictions = model.predict_classes(sequences)\n",
    "    return ohe.categories_[0][predictions]\n",
    "\n",
    "test_tweets = pandas.read_csv('data/test.csv', dtype={'Id': str, 'Tweet': str})\n",
    "test_tweets['Category'] = classify_array(test_tweets.Tweet)\n",
    "\n",
    "with open('predictions_lstm.csv', 'wt') as f:\n",
    "    f.write(\"Id,Category\\n\")\n",
    "    for i, row in test_tweets.iterrows():\n",
    "        f.write(f\"{row.Id},{row.Category}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral     0.5315\n",
      "positive    0.4185\n",
      "negative    0.0500\n",
      "Name: Category, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3980</th>\n",
       "      <td>Trynna go to IHOP but all my friends are asleep so I may just go by myself #WHENINDOUBTPANCAKEITOUT</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3981</th>\n",
       "      <td>IHOP wit bubba jay @GuwopoGodd_ tomorrow  he better eat too</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3982</th>\n",
       "      <td>EMT 09/06/15 [krakatoa]: So when the high point of your Saturday is a chorizo omelette at IHOP for a late dinn... http://t.co/LyXuMPFf8o</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3983</th>\n",
       "      <td>Who's down to go IHOP with me tomorrow morning?</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3984</th>\n",
       "      <td>@GaPeach_est1083 Me either...but we got off early friday so some coworkers went to IHOP...and the rest was history</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3985</th>\n",
       "      <td>definitely going to ihop tomorrow</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3986</th>\n",
       "      <td>Get paid next Friday so I get to buy Koko and I some food from IHOP after the game</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3987</th>\n",
       "      <td>@LebronRuben_ dude I swear Friday I was going to go take you to ihop but Danny had to go home cus he was in pain</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3988</th>\n",
       "      <td>@jbach14 If i didnt have work at 9am tomorrow i really would man. Bring your family to Ihop tomorrow morning and illserve you</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3989</th>\n",
       "      <td>I'm going to IHOP tomorrow.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3990</th>\n",
       "      <td>Not Available</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3991</th>\n",
       "      <td>@__BasedGoddess perfect ! Wednesday I'm done at 1150 . I'll be waiting . Can we go to IHOP one day ?</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3992</th>\n",
       "      <td>Will somebody please get Ihop or rainbow tomorrow with me</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3993</th>\n",
       "      <td>I want to go to IHOP or Friday's</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3994</th>\n",
       "      <td>Anybody with a Steak &amp;amp; Shake or IHOP move tomorrow ??</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>I am assembling an epic Pancake Posse for an IHOP run on Thursday and YOU DON'T EVEN KNOW HOW EXCITED I AM.   ...well, you might know now.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>do you work at Ihop tomorrow @carlysunshine_</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>23 Aug 00;30 #771NAS Rescue193 returned from Tresco, IoS lifted a casualty with minor injuries to RCH Treliske http://t.co/AN6WiE0iIs</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>Not Available</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>IOS 9 App Transport Security. Mm need to check if my 3rd party network pod supports it http://t.co/fmtcfUAdgj</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                           Tweet  \\\n",
       "3980                                         Trynna go to IHOP but all my friends are asleep so I may just go by myself #WHENINDOUBTPANCAKEITOUT   \n",
       "3981                                                                                 IHOP wit bubba jay @GuwopoGodd_ tomorrow  he better eat too   \n",
       "3982    EMT 09/06/15 [krakatoa]: So when the high point of your Saturday is a chorizo omelette at IHOP for a late dinn... http://t.co/LyXuMPFf8o   \n",
       "3983                                                                                             Who's down to go IHOP with me tomorrow morning?   \n",
       "3984                          @GaPeach_est1083 Me either...but we got off early friday so some coworkers went to IHOP...and the rest was history   \n",
       "3985                                                                                                           definitely going to ihop tomorrow   \n",
       "3986                                                          Get paid next Friday so I get to buy Koko and I some food from IHOP after the game   \n",
       "3987                            @LebronRuben_ dude I swear Friday I was going to go take you to ihop but Danny had to go home cus he was in pain   \n",
       "3988               @jbach14 If i didnt have work at 9am tomorrow i really would man. Bring your family to Ihop tomorrow morning and illserve you   \n",
       "3989                                                                                                                 I'm going to IHOP tomorrow.   \n",
       "3990                                                                                                                               Not Available   \n",
       "3991                                        @__BasedGoddess perfect ! Wednesday I'm done at 1150 . I'll be waiting . Can we go to IHOP one day ?   \n",
       "3992                                                                                   Will somebody please get Ihop or rainbow tomorrow with me   \n",
       "3993                                                                                                            I want to go to IHOP or Friday's   \n",
       "3994                                                                                   Anybody with a Steak &amp; Shake or IHOP move tomorrow ??   \n",
       "3995  I am assembling an epic Pancake Posse for an IHOP run on Thursday and YOU DON'T EVEN KNOW HOW EXCITED I AM.   ...well, you might know now.   \n",
       "3996                                                                                                do you work at Ihop tomorrow @carlysunshine_   \n",
       "3997       23 Aug 00;30 #771NAS Rescue193 returned from Tresco, IoS lifted a casualty with minor injuries to RCH Treliske http://t.co/AN6WiE0iIs   \n",
       "3998                                                                                                                               Not Available   \n",
       "3999                               IOS 9 App Transport Security. Mm need to check if my 3rd party network pod supports it http://t.co/fmtcfUAdgj   \n",
       "\n",
       "      Category  \n",
       "3980   neutral  \n",
       "3981  positive  \n",
       "3982  positive  \n",
       "3983   neutral  \n",
       "3984  positive  \n",
       "3985   neutral  \n",
       "3986  positive  \n",
       "3987   neutral  \n",
       "3988  positive  \n",
       "3989   neutral  \n",
       "3990   neutral  \n",
       "3991  positive  \n",
       "3992   neutral  \n",
       "3993   neutral  \n",
       "3994   neutral  \n",
       "3995  positive  \n",
       "3996   neutral  \n",
       "3997   neutral  \n",
       "3998   neutral  \n",
       "3999   neutral  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test_tweets.Category.value_counts(normalize=True))\n",
    "test_tweets[['Tweet', 'Category']].tail(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
